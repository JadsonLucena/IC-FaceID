# -*- coding: utf-8 -*-
"""faceid_beta.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1OynWNoWF6POTcRGFG4V7KW_EGIkUmLYI
# FaceID implementation using face embeddings and RGBD images.
Made by [Norman Di Palo](https://medium.com/@normandipalo), March 2018.
# Let's start by downloading the dataset.
"""



'''
Treinando a Rede Siamesa:

1 - Passe a primeira imagem do par de imagens pela rede.
2 - Passe a segunda imagem do par de imagens pela rede.
3 - Calcule a perda usando as saídas de 1 e 2.
4 - Volte a propagar a perda, para calcular os gradientes.
5 - Atualize os pesos usando um otimizador. Nós usaremos Adam para este exemplo.
'''



import glob, requests, zipfile, io, os, cv2, sklearn, math
import numpy as np
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# https://keras.io/
import keras
from keras.models import Sequential, Model
from keras.layers import Dense, Activation, Flatten, Dropout, Lambda, ELU, concatenate, GlobalAveragePooling2D, Input, BatchNormalization, SeparableConv2D, Subtract, concatenate
from keras.activations import relu, softmax
from keras.layers.convolutional import Convolution2D
from keras.layers.pooling import MaxPooling2D, AveragePooling2D
from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam # Verificar o que é mais eficaz 
from keras.regularizers import l2
from keras import backend as K
from keras.models import load_model
from keras.models import model_from_json


def crawler(dirPath, link_list):
	
	if os.path.isdir(dirPath) == False: # Verifica se o diretório inda não foi criado
		os.mkdir(dirPath) # Caso não exista, cria o diretório

	for link in link_list:
		if os.path.isdir(dirPath + '/' + link.split('/').pop().replace('.zip', '')) == False:  # verifica se o arquivo ainda não foi baixado
			r = requests.get(link, stream=True) # Caso não tenha sido baixado, baixa o arquivo
			z = zipfile.ZipFile(io.BytesIO(r.content)) # Extrai o arquivo
			z.extractall(dirPath) # salva no diretório de treinamento



dir_train = 'faceid_train'
dir_val = 'faceid_val'
dir_model = 'model'
fileType = '.bmp'
depthFileType = '.jpg'
faceCascade = cv2.CascadeClassifier("haarcascade_frontalface_alt.xml")


# Dados de treinamento
#crawler(dir_train, ["http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-16)(151751).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-16)(153054).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-16)(154211).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-16)(160440).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-16)(160931).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-16)(161342).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-16)(163349).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-16)(164248).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-17)(141550).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-17)(142154).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-17)(142457).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-17)(143016).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(132824).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(133201).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(133846).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(134239).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(134757).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(140516).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(143345).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(144316).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(145150).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(145623).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(150303).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(150650).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(151337).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(151650).zip"])
# Dados de validação
#crawler(dir_val, ["http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(152717).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(153532).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(154129).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(154728).zip", "http://vap.aau.dk/wp-content/uploads/VAPRBGD/(2012-05-18)(155357).zip"])


'''
	Pré-processamento de entrada.
	Aqui nós criamos algumas funções que irão criar o par de entrada para o nosso modelo, ambos os pares corretos e errados. Eu criei funções para ter entrada somente de profundidade e entradas RGBD.
'''

def rgbFromDecimal(rgb):
	# return (rgb[0] + 1) + (256*rgb[1]) + (256*256*rgb[2])
	return (round((rgb[0] / 255) * 256) + (256*rgb[1]) + (256*256*rgb[2])) / (256*256*256) # Nomalizado


def margin(coordinates, shape, margin = 100):

	coordinates[0] = coordinates[0] - margin
	if (coordinates[0] < 0):
		coordinates[0] = 0
		
	coordinates[1] = coordinates[1] - margin
	if (coordinates[1] < 0):
		coordinates[1] = 0
		
	coordinates[2] = coordinates[2] + 2 * margin
	if (coordinates[2] > shape[0]):
		coordinates[2] = shape[0]

	coordinates[3] = coordinates[3] + 2 * margin
	if (coordinates[3] > shape[1]):
		coordinates[3] = shape[1]

	return coordinates



def create_input_rgbd(filePath):

	img = cv2.imread(filePath)
		
	depth = cv2.imread(filePath.replace(fileType, depthFileType))
	depth = cv2.resize(depth, (img.shape[1], img.shape[0]))


	facialCoordinates = (0, 0, img.shape[1], img.shape[0]);
	try:
	
		facialCoordinates = margin(faceCascade.detectMultiScale(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), scaleFactor = 1.09, minNeighbors = 1, minSize=(120, 120), flags = cv2.CASCADE_SCALE_IMAGE)[0], img.shape)
	
	except:

		print('\nNão foi possível identificar onde está o rotos: ' + filePath)


	img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
	depth = cv2.cvtColor(depth, cv2.COLOR_BGR2RGB)

	img = img[facialCoordinates[1] : facialCoordinates[1] + facialCoordinates[3], facialCoordinates[0] : facialCoordinates[0] + facialCoordinates[2]]
	depth = depth[facialCoordinates[1] : facialCoordinates[1] + facialCoordinates[3], facialCoordinates[0] : facialCoordinates[0] + facialCoordinates[2]]


	img = cv2.resize(img, (200, 200))
	depth = cv2.resize(depth, (200, 200))

	depth = np.asarray(list(map(lambda line :  list(map(rgbFromDecimal, line)), depth))) # converte os pixels em rgb para decimal

	RGBD = np.zeros((200, 200, 4))
	RGBD[:, :, :3] = img # Insere valores até a terceira dimensão
	RGBD[: , :, 3] = depth # Insere valores de profundidade na quarta dimensão com valores de cada pixel convertidos de RGB para decimal

	return RGBD



rgbdPool = {}
def create_rgbd(person, dir_path, historic = []):

	folder = ''

	if (person == 'same'):
		tmp = np.random.choice(glob.glob(dir_path + "*")) # Pega uma das pastas aleatoriamente
		while tmp == "datalab": 
			tmp = np.random.choice(glob.glob(dir_path + "*"))
		folder = tmp

	# RGBD = np.zeros((2, 200, 200, 4)) # Cria 2 matrizez de zeros 200 X 200 com 4 dimensões
	RGBD = []

	for _ in range(2): # Cria 2 matrizez de zeros 200 X 200 com 4 dimensões

		if (person == 'different'):
			tmp = np.random.choice(glob.glob(dir_path + "*")) # Pega uma das pastas aleatoriamente
			while tmp == folder or tmp == "datalab": 
				tmp = np.random.choice(glob.glob(dir_path + "*"))
			folder = tmp

		filePath = np.random.choice(glob.glob(folder + "/*"+ fileType)) # Pega um dos arquivos aleatoriamente
		while filePath in historic:
			filePath = np.random.choice(glob.glob(folder + "/*"+ fileType))
		
		historic += filePath

		if (not (filePath in rgbdPool)):
			rgbdPool[filePath] = create_input_rgbd(filePath)

		RGBD.append(rgbdPool[filePath])


	return {'RGBD' : np.array(RGBD), 'historic' : historic}


# create_rgbd('same', dir_val + '/')['RGBD'][0].shape
# create_rgbd('different', dir_val + '/')['RGBD'][0].shape




'''
	Elaboração de redes (rede convolucional siamês baseada na arquitetura SqueezeNet com perda contrastiva)
	Agora nós criamos a rede. Primeiro criamos manualmente * a perda contrastiva *, depois definimos a arquitetura da rede a partir da arquitetura SqueezeNet, e depois a usamos como uma rede siamesa para incorporar faces em um manifold. (a rede, por enquanto, é muito grande e pode ser muito otimizada, mas eu só queria mostrar uma prova de conceito)
'''

def euclidean_distance(inputs):
	
	assert len(inputs) == 2, 'Euclidean distance needs 2 inputs, %d given' % len(inputs)
	u, v = inputs
	
	return K.sqrt(K.sum((K.square(u - v)), axis=1, keepdims=True))
        


def contrastive_loss(y_true, y_pred):

	margin = 1.

	return K.mean((1. - y_true) * K.square(y_pred) + y_true * K.square(K.maximum(margin - y_pred, 0.)))
	# return K.mean( K.square(y_pred) )


# Expande as dimenções da convolução
def fire(x, squeeze = 16, expand = 64):
    
    x = Convolution2D(squeeze, (1,1), padding='valid')(x)
    x = Activation('relu')(x)
    
    left = Convolution2D(expand, (1,1), padding='valid')(x)
    left = Activation('relu')(left)
    
    right = Convolution2D(expand, (3,3), padding='same')(x)
    right = Activation('relu')(right)
    
    x = concatenate([left, right], axis = 3)

    return x


# Fase de aprendizagem: Nós escrevemos os geradores que darão ao nosso modelo lotes de dados (manifold) para treinar, então nós executamos o treinamento
def generatorBatch(batch_size, dir_path):

	while True:
		
		historic = []
		X = []
		y = []
		switch = True
		
		for _ in range(batch_size):

			if switch:
				# print("same")

				same_rgbd = create_rgbd('same', dir_path + '/', historic)

				historic = same_rgbd['historic']

				X.append(same_rgbd['RGBD'].reshape((2,200,200,4)))
				y.append(np.array([0.]))

			else:
				# print("different")

				different_rgbd = create_rgbd('different', dir_path + '/', historic)

				historic = different_rgbd['historic']

				X.append(different_rgbd['RGBD'].reshape((2,200,200,4)))
				y.append(np.array([1.]))
			
			switch = not switch

		X = np.asarray(X)
		y = np.asarray(y)
		
		XX1 = X[0, :]
		XX2 = X[1, :]

		yield [ X[:, 0], X[:, 1] ], y



def siameseNetworkSqueezeNet(config):

	img_input = Input(shape = (200, 200, 4))

	x = Convolution2D(64, (5, 5), strides = (2, 2), padding = 'valid')(img_input) # (dimensionalidade do espaço de saída, largura da janela de convolução 2D, os passos da convolução ao longo da altura e largura, padding)
	x = BatchNormalization()(x) # Normaliza entre 0 e 1
	x = Activation('relu')(x) # função de ativação
	x = MaxPooling2D(pool_size = (3, 3), strides = (2, 2))(x) # (irá reduzir pela 1/3 a entrada em ambas as dimensões espaciais, largura da janela de convolução 2D)

	x = fire(x, squeeze = 16, expand = 16)
	x = fire(x, squeeze = 16, expand = 16)

	x = MaxPooling2D(pool_size = (3, 3), strides = (2, 2))(x)

	x = fire(x, squeeze = 32, expand = 32)
	x = fire(x, squeeze = 32, expand = 32)

	x = MaxPooling2D(pool_size = (3, 3), strides = (2, 2))(x)

	x = fire(x, squeeze = 48, expand = 48)
	x = fire(x, squeeze = 48, expand = 48)

	x = fire(x, squeeze = 64, expand = 64)
	x = fire(x, squeeze = 64, expand = 64)

	x = Dropout(0.2)(x) # Taxa do treinamento

	x = Convolution2D(512, (1, 1), padding = 'same')(x)
	out = Activation('relu')(x) # Calculo retificado linear

	modelsqueeze = Model(img_input, out) # Agrupa camadas em um objeto com recursos de treinamento e inferência

	modelsqueeze.summary() # Imprime um resumo de cadeia da rede




	im_in = Input(shape = (200,200,4))
	#wrong = Input(shape = (200,200,3))

	x1 = modelsqueeze(im_in)

	#x = Convolution2D(64, (5, 5), padding='valid', strides =(2,2))(x)
	#x1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x1)

	'''
	x1 = Convolution2D(256, (3,3), padding='valid', activation="relu")(x1)
	x1 = Dropout(0.4)(x1)
	x1 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1))(x1)
	x1 = Convolution2D(256, (3,3), padding='valid', activation="relu")(x1)
	x1 = BatchNormalization()(x1)
	x1 = Dropout(0.4)(x1)
	x1 = Convolution2D(64, (1,1), padding='same', activation="relu")(x1)
	x1 = BatchNormalization()(x1)
	x1 = Dropout(0.4)(x1)
	'''



	x1 = Flatten()(x1) # Aplaina a entrada

	x1 = Dense(512, activation = "relu")(x1) # camada NN normal e densamente conectada
	x1 = Dropout(0.2)(x1)
	#x1 = BatchNormalization()(x1)
	feat_x = Dense(128, activation = "linear")(x1)
	feat_x = Lambda(lambda  x: K.l2_normalize(x, axis = 1))(feat_x)


	model_top = Model(inputs = [im_in], outputs = feat_x)

	model_top.summary()

	im_in1 = Input(shape = (200,200,4))
	im_in2 = Input(shape = (200,200,4))

	feat_x1 = model_top(im_in1)
	feat_x2 = model_top(im_in2)


	lambda_merge = Lambda(euclidean_distance)([feat_x1, feat_x2])


	model_final = Model(inputs = [im_in1, im_in2], outputs = lambda_merge)

	model_final.summary()

	model_final.compile(optimizer = config['compile']['optimizer'], loss = config['compile']['loss']) # (nome do otimizador, nome da função objetivo) Configura o modelo para treinamento.


	'''
	Fase de aprendizagem
	Nós escrevemos os geradores que darão ao nosso modelo lotes de dados para treinar, então nós executamos o treinamento
	'''

	train_gen = generatorBatch(16, dir_train)
	val_gen = generatorBatch(4, dir_val)


	# Treina o modelo em dados gerados lote a lote por um gerador
	model_final.fit_generator(train_gen, steps_per_epoch = config['fit']['steps_per_epoch'], epochs = config['fit']['epochs'], validation_data = val_gen, validation_steps = config['fit']['validation_steps'])



	''' Alguns testes de modelo '''
	cop = create_rgbd('same', dir_val + '/')['RGBD']
	model_final.evaluate([cop[0].reshape((1, 200, 200, 4)), cop[1].reshape((1, 200, 200, 4))], np.array([0.]))

	cop = create_rgbd('different', dir_val + '/')['RGBD']
	model_final.predict([cop[0].reshape((1, 200, 200, 4)), cop[1].reshape((1, 200, 200, 4))])




	'''
		Saída bruta
		Aqui nós criamos um modelo que gera a incorporação de uma face de entrada em vez da distância entre dois envoltórios, para que possamos mapear essas saídas.
	'''

	# im_in1 = Input(shape = (200, 200, 4))
	#im_in2 = Input(shape=(200,200,4))

	# feat_x1 = model_top(im_in1)
	#feat_x2 = model_top(im_in2)

	model_output = Model(inputs = im_in1, outputs = feat_x1)

	model_output.summary()

	model_output.compile(optimizer = config['compile']['optimizer'], loss = config['compile']['loss']) # Saida da rede siamesa

	# cop = create_rgbd('same', dir_val + '/')['RGBD']
	model_output.predict(cop[0].reshape((1, 200, 200, 4)))


	'''
		Visualização de dados
		Aqui, armazenamos as integrações para todos os rostos no conjunto de dados. Em seguida, usando o ** t-SNE ** e o ** PCA **, visualizamos as integrações que vão de 128 a 2 dimensões.

		Visualização de dados (algoritmo t-SNE)
	'''

	outputs = []
	n = 0
	for folder in glob.glob(dir_train + '/*'):
		
		i = 0
		
		for file in glob.glob(folder + "/*" + fileType):
			
			outputs.append(model_output.predict(create_input_rgbd(file).reshape((1, 200, 200, 4))))
			
			i += 1
		
		
		n += 1
		
		print("Folder ", n, " of ", len(glob.glob(dir_train + '/*')), "with", i, "files")

	print(len(outputs))


	outputs = np.asarray(outputs)
	outputs = outputs.reshape((-1,128))
	outputs.shape


	X_embedded = TSNE(2).fit_transform(outputs)
	X_embedded.shape

	X_PCA = PCA(3).fit_transform(outputs)
	print(X_PCA.shape)

	#X_embedded = TSNE(2).fit_transform(X_PCA)
	#print(X_embedded.shape)

	color = 0
	for i in range(len((X_embedded))):
		
		el = X_embedded[i]
		
		if i % 51 == 0 and not i == 0:
			color += 1
			color = color % 10
		
		plt.scatter(el[0], el[1], color = "C" + str(color))

	plt.show()



	return model_final



'''
	Distância entre duas imagens RGBD arbitrárias
'''

def showResult(face, test):

	face = cv2.cvtColor(cv2.imread(face), cv2.COLOR_BGR2RGB)

	testFaces = list(map(lambda path : cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB), [face[0] for face in test]))

	plt.figure(figsize = (12, 10))

	for i in range(len(testFaces)):

		newImg = np.concatenate((face, testFaces[i]), axis=1)
		
		squareDimension = math.ceil(math.sqrt(len(testFaces)))
		plt.subplot(squareDimension, squareDimension, i + 1)
		plt.imshow(newImg)
		plt.title(test[i][1])
		plt.axis('off')

	plt.show()



def validation(model):

	people = glob.glob(dir_train + "/*") # Pega todas as pastas de treinamento

	person = np.random.choice(people) # pega uma pessoa aleatoriamente para servir como modelo
	face = np.random.choice(glob.glob(person + "/*" + fileType)) # Pega uma imagem aleatória da pessoa 

	people.remove(person) # remove a pessoa que será usada como modelo

	
	# Testa com a primeira imagem

	faceTest = np.random.choice(glob.glob(person + "/*" + fileType)) # Pega uma outra imagem aleatória da pessoa 
	while faceTest == face: # Evita pegar a mesma imagem
		faceTest = np.random.choice(glob.glob(person + "/*" + fileType))


	showResult(face, [(faceTest, model.predict([create_input_rgbd(face).reshape((1, 200, 200, 4)), create_input_rgbd(faceTest).reshape((1, 200, 200, 4))]))])

	
	# Testa com as imagens Incorretas 

	tests = []
	for person in people: # Faz o teste com todas as outras pessoas
		faceTest = np.random.choice(glob.glob(person + "/*" + fileType))
		tests.append((faceTest, model.predict([create_input_rgbd(face).reshape((1, 200, 200, 4)), create_input_rgbd(faceTest).reshape((1, 200, 200, 4))])))

	showResult(face, tests)



def test(model):

	for person in glob.glob(dir_val + "/*"):

		face = np.random.choice(glob.glob(person + "/*" + fileType)) # Pega uma imagem aleatória da pessoa 
		
		tests = []
		for person in glob.glob(dir_train + "/*"): # Testa com as imagens de treinamento 
			faceTest = np.random.choice(glob.glob(person + "/*" + fileType))
			tests.append((faceTest, model.predict([create_input_rgbd(face).reshape((1, 200, 200, 4)), create_input_rgbd(faceTest).reshape((1, 200, 200, 4))])))

		showResult(face, tests)



'''
Fluxo do algoritmo
'''

# modelFile = 'model.h5'

# model = '';
# if os.path.exists(dir_model + '/' + modelFile) and input('Você deseja utilizar um modelo salvo? [Y/N]: ').upper() == 'Y':
	# model = load_model(dir_model + '/' + modelFile)

# else:

model = siameseNetworkSqueezeNet({
	'compile' : {
		'optimizer'	: Adam(lr = 0.001),	# SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam
		# 'optimizer'	: SGD(lr = 0.001, momentum = 0.9), # pode ser usado no lugar do adam
		'loss'		: contrastive_loss	# absolute_difference, add_loss, compute_weighted_loss, cosine_distance, get_losses, get_regularization_loss, get_regularization_losses, get_total_loss, hinge_loss, huber_loss, log_loss, mean_pairwise_squared_error, mean_squared_error, sigmoid_cross_entropy, softmax_cross_entropy, sparse_softmax_cross_entropy
	},
	'fit' : {
		'steps_per_epoch'	: 30,
		'epochs'			: 50,
		'validation_steps'	: 20
	}
})

	# if not os.path.isdir(dir_model):
		# os.mkdir(dir_model)
	
	# if (not os.path.exists(dir_model + '/' + modelFile)) or (os.path.exists(dir_model + '/' + modelFile) and input('Você deseja substituir o modelo salvo existente? [Y/N]: ').upper() == 'Y'):
		# model.save(dir_model + '/' + modelFile)


validation(model)
test(model)
